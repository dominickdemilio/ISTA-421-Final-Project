{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Johnny Appleseed model just to play with it a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Weights:  [-0.08108529  4.44161259  1.66889345]\n",
      "Bias:  1.9071065862991954\n",
      "Predicted population growth for new areas:  [95.52693676 65.35099395 16.70224028]\n",
      "He would have planted here. Population pred: 95.5\n",
      "He would have planted here. Population pred: 65.4\n",
      "Johnny Appleseed will not plant in this area. Population pred: 16.7\n"
     ]
    }
   ],
   "source": [
    "# Example of Linear Regression\n",
    "# Linear Regression using sklearn\n",
    "model = LinearRegression()\n",
    "\n",
    "# X = [distance_to_water, soil_quality, current_population]\n",
    "X = np.array([\n",
    "    [2,7,50],\n",
    "    [5,5,30],\n",
    "    [1,9,60],\n",
    "    [8,4,20],\n",
    "    [30,1,5]\n",
    "])\n",
    "\n",
    "# Target: Expected population in 5 years\n",
    "y = np.array([100, 80, 150, 60, 10])\n",
    "\n",
    "# Number of training examples and features\n",
    "n_samples, n_features = X.shape\n",
    "X.shape\n",
    "\n",
    "# weights and bias\n",
    "weights = np.zeros(n_features)\n",
    "bias = 1\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "\n",
    "# Gradient Descent\n",
    "for epoch in range(epochs):\n",
    "    y_predicted = np.dot(X, weights) + bias\n",
    "\n",
    "    # gradients\n",
    "    dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "    db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "    # weights and biases\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "\n",
    "\n",
    "# Test data\n",
    "X_new = np.array([\n",
    "    [4,8,35],\n",
    "    [6,5,25],\n",
    "    [30,2,5]\n",
    "])\n",
    "new_pred = np.dot(X_new, weights) + bias\n",
    "\n",
    "print(\"Weights: \", weights)\n",
    "print(\"Bias: \", bias)\n",
    "print(\"Predicted population growth for new areas: \", new_pred)\n",
    "\n",
    "for new in new_pred:\n",
    "    if new < 50:\n",
    "        print(\n",
    "            f\"Johnny Appleseed will not plant in this area. Population pred: {np.round(new, 1)}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"He would have planted here. Population pred: {np.round(new, 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning of my own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "filename = \"dataset/survey_results_public.csv\"\n",
    "df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "# Encode categorical data\n",
    "education_encoding_map = {\n",
    "    \"I never completed any formal education\": 0,\n",
    "    \"Primary/elementary school\": 1,\n",
    "    \"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\": 2,\n",
    "    \"Some college/university study without earning a degree\": 3,\n",
    "    \"Associate degree\": 4,\n",
    "    \"Bachelor's degree (BA, BS, B.Eng., etc.)\": 5,\n",
    "    \"Master's degree (MA, MS, M.Eng., MBA, etc.)\": 6,\n",
    "    \"Professional degree (JD, MD, etc.)\": 7,\n",
    "    \"Other doctoral degree (Ph.D, Ed.D., etc.)\": 8\n",
    "}\n",
    "years_coding_encoding_map = {\n",
    "    '0-2 years': 0,\n",
    "    '3-5 years': 1,\n",
    "    '6-8 years': 2,\n",
    "    '9-11 years': 3,\n",
    "    '12-14 years': 4,\n",
    "    '15-17 years': 5,\n",
    "    '18-20 years': 6,\n",
    "    '21-23 years': 7,\n",
    "    '24-26 years': 8,\n",
    "    '27-29 years': 9,\n",
    "    '30 or more years': 10\n",
    "}\n",
    "hours_computer_encoding_mapping = {\n",
    "    'Less than 1 hour': 0,\n",
    "    '1 - 4 hours': 1,\n",
    "    '5 - 8 hours': 2,\n",
    "    '9 - 12 hours': 3,\n",
    "    'Over 12 hours': 4\n",
    "}\n",
    "df['FormalEducation'] = df['FormalEducation'].map(education_encoding_map)\n",
    "df[\"YearsCoding\"] = df[\"YearsCoding\"].map(years_coding_encoding_map)\n",
    "df[\"HoursComputer\"] = df[\"HoursComputer\"].map(hours_computer_encoding_mapping)\n",
    "\n",
    "# Filter columns and remove NULLS\n",
    "cols = [\"FormalEducation\", \"YearsCoding\", \"HoursComputer\", \"ConvertedSalary\"]\n",
    "df_encoded = df[cols]\n",
    "df_encoded = df_encoded.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y variables\n",
    "X = df_encoded.drop(\"ConvertedSalary\", axis=1)\n",
    "y = df_encoded[\"ConvertedSalary\"]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)\n",
    "\n",
    "# Convert to numpy arrays \n",
    "X_train_np = X_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "# samples and features\n",
    "n_samples, n_features = X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights/bias\n",
    "weights = np.zeros(n_features)\n",
    "bias = 1\n",
    "learning_rate = 0.001\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "for epoch in range(epochs):\n",
    "    y_predicted = np.dot(X_train_np, weights) + bias\n",
    "\n",
    "    # gradients\n",
    "    dw = (1 / n_samples) * np.dot(X_train_np.T, (y_predicted - y_train_np))\n",
    "    db = (1 / n_samples) * np.sum(y_predicted - y_train_np)\n",
    "\n",
    "    # weights and biases\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  39698088820.43863\n",
      "Weights:  [ 4335.44745063 10431.98916744  9056.27473919]\n",
      "Bias:  13913.755939945793\n",
      "New y predictions:  [40697.20031958 55464.63693765 95816.87917917 ... 82425.15698935\n",
      " 52504.90391528 94441.16475091]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.dot(X_test_np, weights) + bias\n",
    "mse_test = np.mean((y_test_pred - y_test_np) ** 2)\n",
    "print(\"MSE: \", mse_test)\n",
    "\n",
    "print(\"Weights: \", weights)\n",
    "print(\"Bias: \", bias)\n",
    "print(\"New y predictions: \", y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see here the weights and biases associated with each coefficient in my simple model. I will work to add more features in the future, but for now we can see that of the 3 independent variables I included (Formal Education, # Years Coding, Hours/Day Spent Coding), formal education is surprisingly the least relevant when it comes to predicting salary. This goes against my original hypothesis that it would be very relevant. Instead, experience seems to be the biggest contributor to \"ConvertedSalary\", as we see that that feature's weight is roughly 2.5 times greater than education. Additionally, Hours/Day is weight quite strongly, which is less surprising to me. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Questions\n",
    "\n",
    "## Question 1\n",
    "\n",
    "#### RSS (Residual Sum of Squares) represents the total squared difference between predicted values and actual data points, while RSE (Residual Standard Error) is the root of the average squared distance, basically acting as a standardized measure of how well the regression line fits the data. RSS is a raw sum, while RSE is a standardized measure of the average error per data point.\n",
    "\n",
    "#### RSS is given by: $$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$ where n is the number of data points, y_i is the observed response for the ith datapoint, and y_hat_i is the predicted value from the model for that point.\n",
    "\n",
    "#### RSE is given by: $$ \\text{RSE} = \\sqrt{ \\frac{1}{n - p} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 } $$ where p is the number of parameters. This is basically an estimated standard deviation of the model's residuals, measuring how far residuals tend to deviate from 0.\n",
    "\n",
    "#### RSS is often used as a way to evaluate and calculate other metrics. For example, R^2 depends on RSS. On the other hand, RSE is a more intuitive measure which is not really used to calculate other metrics. RSE uses the same units as the output variable, where RSS is the unit squared. So while they both measure error, RSS measures the overall size of the errors while RSE transforms that sum into a standard deviation of residuals.\n",
    "\n",
    "## Question 2\n",
    "\n",
    "#### A loss function quantifies the discrepancy between the model's predictions and the actuals. You can choose a loss function with respect to the model's parameters. \n",
    "\n",
    "#### Mean Squared Error (MSE) is a specific type of loss function seen often in regression models. Its equation is $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} \\bigl(y_i - \\hat{y}_i\\bigr)^2 $$\n",
    "\n",
    "#### MSE is mainly used for regression problems, with an emphasis on penalizing large errors. This function squares residuals, which heavily weights bigger mistakes. lower MSE = predictions are closer on average than higher MSE \n",
    "\n",
    "#### In general, a loss function will tell you how poorly a model is doing. The primary object of training the model is to reduce this function; if the model is doing 'less bad', it must improving. In summary, MSE is one of the simplest and most common loss functions for regression.\n",
    "\n",
    "## Question 3\n",
    "\n",
    "#### To the untrained eye, linear regression may seem straightforward: modeling the relationship between a dependent variable Y and one or more independent variables X, using a simple linear equation that looks strikingly similar to the first equation many of us learned: y=mx+b. However, the complexity in linear regression arises from a number of factors. One is all the assumptions you have to make. Linear regression relies on key assumptions like linearity, independence of variables, and normality of errors. Many models are also prone to multicollinearity, when predictors are highly correlated. This affects the regression coefficients making interpretation of the model very difficult. Even with simple models, finding a good balance such that the model is not over or under fitting is very important. Understanding these complexities on a fundamental level is essential for effectively applying regression techniques.\n",
    "\n",
    "## Question 4\n",
    "\n",
    "#### There are several techniques and strategies you can employ to determine if a model is a good or bad fit. One that comes to mind is an R^2 test, which measures the proportion of variance in the dependent variable explained by the independent variables. R^2 ranges from 0..1, and this serves as a key metric of the model's accuracy. The closer R^2 is to 1, the better the fit of the model. However, one thing to keep in mind here is, R^2 could be high even if the model has been overfit to the data, so this cannot be the only method used to assess the quality of the model. Another method is plotting residuals around zero and making sure the distribution is randomly scattered, rather than having clear patterns or clustering. You can also use error metrics like Mean Squared Error (MSE). Like I previously discussed, having a lower MSE is more desirable and shows that the model is improving (better fit). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
